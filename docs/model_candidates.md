# 模型候选调研报告

## 1. 序列建模类
- **Temporal Fusion Transformer (TFT)**：适合多变量时间序列，能处理缺失与多头注意力。优点是对时间/静态特征友好，支持量化不确定性；缺点是实现复杂、训练成本高，对我们现阶段的快速迭代不够友好。
- **Informer / Autoformer / PatchTST**：面向长序列的 Transformer 族，可处理高频金融数据，但需要较大的 batch 与 GPU 支持，同时对特征工程要求较高。
- **N-BEATS / N-HiTS**：面向单变量预测，难以直接利用 1k+ 的截面特征，不适合作为主力模型。

## 2. 表格深度模型
- **TabNet / TabTransformer**：对异构表格数据效果稳定，具备注意力解释性。但需要 Pytorch/PyTorch-Lightning 等支撑，训练调优较耗时。
- **FT-Transformer**：对结构化数据的 Transformer 方案，需较长训练周期和精细调参。

## 3. 树模型与混合策略
- **CatBoost**：基于对称树 + 有序目标编码，缺省可应对缺失值和类别特征，对表格问题非常稳健；对我们的高维特征适配度高，且易于与现有 LightGBM 集成。
- **XGBoost / LightGBM (现有)**：作为基线稳定，但单模型增益有限，需要引入新模型或 stacking。
- **线性/岭回归 (现有)**：作为平滑补充，但预测力有限。

## 4. 本轮选择
综合考虑实现复杂度、与现有流水线的兼容性以及对高维特征的适应能力，本轮优先引入 **CatBoost 回归模型** 作为新增主力学习器：
- 易于集成，API 与 LightGBM 相近，可直接输出回归值参与集成。
- 内置处理缺失值、列变换，对新增的派生特征更鲁棒。
- 模块化集成后，可作为后续深度模型的桥梁，为 stacking/ensemble 构建打基础。

后续若 CatBoost 仍不足以达到目标，将优先尝试 TabTransformer/TFT 等深度序列模型，并在该文档持续记录决策。
